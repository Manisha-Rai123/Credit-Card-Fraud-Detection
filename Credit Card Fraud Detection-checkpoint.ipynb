{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33867709-b492-4b33-ba8f-43762acd443c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
      "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
      "\n",
      "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
      "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
      "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
      "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
      "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
      "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
      "\n",
      "        V26       V27       V28  Amount  Class  \n",
      "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
      "1  0.125895 -0.008983  0.014724    2.69      0  \n",
      "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
      "3 -0.221929  0.062723  0.061458  123.50      0  \n",
      "4  0.502292  0.219422  0.215153   69.99      0  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n",
      "None\n",
      "(284315, 31)\n",
      "(492, 31)\n",
      "            Time        V1        V2        V3        V4        V5        V6  \\\n",
      "140702   83879.0 -1.376888  0.304186  2.232899 -0.747521 -0.723582 -0.507505   \n",
      "33702    37345.0  1.258964 -0.171092  0.591029 -0.020344 -0.813602 -0.833055   \n",
      "14611    25824.0 -3.758598  2.671649 -1.095451 -2.026820 -0.379909 -0.395385   \n",
      "219032  141568.0  0.610701 -2.715727 -1.066648  0.859169 -1.059923  0.844772   \n",
      "21534    31726.0 -1.824822  0.134345  0.202888  1.816549 -2.257433  1.202735   \n",
      "...          ...       ...       ...       ...       ...       ...       ...   \n",
      "279863  169142.0 -1.927883  1.125653 -4.518331  1.749293 -1.566487 -2.010494   \n",
      "280143  169347.0  1.378559  1.289381 -5.004247  1.411850  0.442581 -1.326536   \n",
      "280149  169351.0 -0.676143  1.126366 -2.213700  0.468308 -1.120541 -0.003346   \n",
      "281144  169966.0 -3.113832  0.585864 -5.399730  1.817092 -0.840618 -2.943548   \n",
      "281674  170348.0  1.991976  0.158476 -2.583441  0.408670  1.151147 -0.096695   \n",
      "\n",
      "              V7        V8        V9  ...       V20       V21       V22  \\\n",
      "140702 -0.240224  0.579009  0.339991  ... -0.028917 -0.098684 -0.399710   \n",
      "33702  -0.190881 -0.141911  0.651640  ... -0.044670 -0.334207 -0.809029   \n",
      "14611  -0.232324  0.767746  1.482821  ...  0.357693 -0.275608 -0.407692   \n",
      "219032  0.104710  0.090206  1.522520  ...  1.230240  0.234188 -0.569519   \n",
      "21534   3.400232 -1.168817  0.353718  ... -1.036254 -0.538060  1.018867   \n",
      "...          ...       ...       ...  ...       ...       ...       ...   \n",
      "279863 -0.882850  0.697211 -2.064945  ...  1.252967  0.778584 -0.319189   \n",
      "280143 -1.413170  0.248525 -1.127396  ...  0.226138  0.370612  0.028234   \n",
      "280149 -2.234739  1.210158 -0.652250  ...  0.247968  0.751826  0.834108   \n",
      "281144 -2.208002  1.058733 -1.632333  ...  0.306271  0.583276 -0.269209   \n",
      "281674  0.223050 -0.068384  0.577829  ... -0.017652 -0.164350 -0.295135   \n",
      "\n",
      "             V23       V24       V25       V26       V27       V28  Amount  \n",
      "140702 -0.135867  0.453108  0.179617  0.856157 -0.141489 -0.060409   26.51  \n",
      "33702   0.117966  0.476777  0.119501  1.201376 -0.093173  0.002534   11.71  \n",
      "14611   0.178739 -0.993128  0.291580  0.745907 -0.623763 -0.204866    1.46  \n",
      "219032 -0.329840  0.368090 -0.453956 -0.968233 -0.054184  0.087021  695.72  \n",
      "21534   0.504384  0.487405 -0.103717 -0.150849  0.031524 -1.190140  577.00  \n",
      "...          ...       ...       ...       ...       ...       ...     ...  \n",
      "279863  0.639419 -0.294885  0.537503  0.788395  0.292680  0.147968  390.00  \n",
      "280143 -0.145640 -0.081049  0.521875  0.739467  0.389152  0.186637    0.76  \n",
      "280149  0.190944  0.032070 -0.739695  0.471111  0.385107  0.194361   77.89  \n",
      "281144 -0.456108 -0.183659 -0.328168  0.606116  0.884876 -0.253700  245.00  \n",
      "281674 -0.072173 -0.450261  0.313267 -0.289617  0.002988 -0.015309   42.53  \n",
      "\n",
      "[984 rows x 30 columns]\n",
      "140702    0\n",
      "33702     0\n",
      "14611     0\n",
      "219032    0\n",
      "21534     0\n",
      "         ..\n",
      "279863    1\n",
      "280143    1\n",
      "280149    1\n",
      "281144    1\n",
      "281674    1\n",
      "Name: Class, Length: 984, dtype: int64\n",
      "            Time        V1        V2         V3         V4        V5  \\\n",
      "42007    40918.0 -3.140260  3.367342  -2.778931   3.859701 -1.159518   \n",
      "138235   82554.0  0.982565 -1.430579   0.765007  -0.299148 -1.524610   \n",
      "41227    40595.0 -0.928282 -0.333968   0.980840  -1.769533  0.905853   \n",
      "160092  113116.0  2.092764 -0.140174  -1.555249   0.156985  0.150913   \n",
      "169728  119830.0  2.287905 -0.586651  -1.754496  -1.171346  0.148971   \n",
      "...          ...       ...       ...        ...        ...       ...   \n",
      "10568    17520.0 -5.268053  9.067613 -15.960728  10.296603 -4.708241   \n",
      "27749    34687.0 -0.860827  3.131790  -5.052968   5.420941 -2.494141   \n",
      "623        472.0 -3.043541 -3.157307   1.088463   2.288644  1.359805   \n",
      "9252     13323.0 -5.454362  8.287421 -12.752811   8.594342 -3.106002   \n",
      "180759  124687.0  1.617641 -1.178338  -1.885251   0.760856 -0.259888   \n",
      "\n",
      "              V6         V7        V8        V9  ...       V20       V21  \\\n",
      "42007  -0.721552  -4.195342 -0.598346 -2.870145  ...  0.077781  2.452339   \n",
      "138235  0.030034  -0.819261 -0.002072  0.062534  ...  0.514050  0.099297   \n",
      "41227  -1.288849   0.297423 -0.152014 -2.055005  ...  0.344660  0.382274   \n",
      "160092 -0.857760   0.060048 -0.198269  0.990572  ... -0.316787  0.235550   \n",
      "169728 -0.552811  -0.259617 -0.387906 -0.750202  ...  0.131193  0.397875   \n",
      "...          ...        ...       ...       ...  ...       ...       ...   \n",
      "10568  -3.395375 -11.161057  5.499963 -5.667376  ...  1.455878  2.004110   \n",
      "27749  -1.811287  -5.479117  1.189472 -3.908206  ...  1.085760  1.192694   \n",
      "623    -1.064823   0.325574 -0.067794 -0.270953  ...  2.102339  0.661696   \n",
      "9252   -3.179949  -9.252794  4.245062 -6.329801  ...  1.305862  1.846165   \n",
      "180759 -0.778156   0.470034 -0.457674 -0.908504  ... -0.098880 -0.104787   \n",
      "\n",
      "             V22       V23       V24       V25       V26       V27       V28  \\\n",
      "42007  -0.292963 -0.189330 -0.166482  0.038040 -0.015477  0.776691  0.397557   \n",
      "138235  0.029484 -0.241394 -0.059906  0.393073 -0.236685  0.018445  0.056836   \n",
      "41227   0.725178 -0.243433  0.064615  0.509555 -0.237828  0.007140  0.089847   \n",
      "160092  0.754853  0.009563  0.559198  0.325546 -0.438149 -0.006408 -0.051915   \n",
      "169728  1.217375 -0.091325  0.138751  0.379857  0.073063 -0.034083 -0.062302   \n",
      "...          ...       ...       ...       ...       ...       ...       ...   \n",
      "10568   0.191058  0.622928 -1.209264 -0.374799  0.648798  1.584697  0.720056   \n",
      "27749   0.090356 -0.341881 -0.215924  1.053032  0.271139  1.373300  0.691195   \n",
      "623     0.435477  1.375966 -0.293803  0.279798 -0.145362 -0.252773  0.035764   \n",
      "9252   -0.267172 -0.310804 -1.201685  1.352176  0.608425  1.574715  0.808725   \n",
      "180759 -0.321156 -0.185033 -0.451996  0.171986 -0.519211 -0.035889 -0.006020   \n",
      "\n",
      "        Amount  \n",
      "42007     0.76  \n",
      "138235  213.76  \n",
      "41227    25.00  \n",
      "160092    1.00  \n",
      "169728   15.00  \n",
      "...        ...  \n",
      "10568     1.00  \n",
      "27749    19.02  \n",
      "623     529.00  \n",
      "9252      1.00  \n",
      "180759  292.13  \n",
      "\n",
      "[787 rows x 30 columns]\n",
      "Accuracy on Training data : 0.9479034307496823\n",
      "Accuracy on Test data : 0.934010152284264\n",
      "Model and scaler saved.\n",
      "Predicted Class: [0]\n",
      "Legitimate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Desktop\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Administrator\\Desktop\\Anaconda\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Administrator\\Desktop\\Anaconda\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#importing the Dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "import joblib\n",
    "\n",
    "#Loading the dataset to a Pandas DataFrame\n",
    "a=pd.read_csv(r\"C:\\Users\\Administrator\\Downloads\\creditcard.csv\\creditcard.csv\")\n",
    "\n",
    "#first 5rows of the dataset\n",
    "print(a.head())\n",
    "\n",
    "#Dataset information\n",
    "print(a.info())\n",
    "\n",
    "#Checking the number of missing values in each columns\n",
    "a.isnull().sum()\n",
    "\n",
    "#distribution of legit transaction & fraudulant transaction\n",
    "a['Class'].value_counts()\n",
    "\n",
    "#seperating the data for analysis 0--->Normal transaction, 1--->fraudulant transaction\n",
    "legit=a[a.Class==0]\n",
    "fraud=a[a.Class==1]\n",
    "print(legit.shape)\n",
    "print(fraud.shape)\n",
    "\n",
    "#satatical message of the data\n",
    "legit.Amount.describe()\n",
    "fraud.Amount.describe()\n",
    "\n",
    "#compare the value for both transactions\n",
    "a.groupby('Class').mean()\n",
    "legit_sample=legit.sample(n=492)\n",
    "\n",
    "#concating the DataFrame\n",
    "new_dataset=pd.concat([legit_sample,fraud],axis=0)\n",
    "new_dataset.head()\n",
    "new_dataset['Class'].value_counts()\n",
    "\n",
    "#spliting the data int features & target\n",
    "x=new_dataset.drop(columns='Class',axis=1)\n",
    "y=new_dataset['Class']\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "# Scaling the features\n",
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(x)\n",
    "\n",
    "\n",
    "#split the data into Training & Testing data\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=2)\n",
    "print(x_train)\n",
    "\n",
    "#Model Training\n",
    "model=LogisticRegression()\n",
    "\n",
    "#training the logistic regression model with training data\n",
    "model.fit(x_train,y_train)\n",
    "\n",
    "#Accuracy on Training data\n",
    "x_train_prediction=model.predict(x_train)\n",
    "training_data_accuracy=accuracy_score(x_train_prediction,y_train)\n",
    "print(\"Accuracy on Training data :\",training_data_accuracy)\n",
    "\n",
    "#Accuracy on Test data\n",
    "x_test_prediction=model.predict(x_test)\n",
    "test_data_accuracy=accuracy_score(x_test_prediction,y_test)\n",
    "print(\"Accuracy on Test data :\",test_data_accuracy)\n",
    "\n",
    "\n",
    "joblib.dump(model, 'credit_card_fraud_model.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "print(\"Model and scaler saved.\")\n",
    "\n",
    "# Creating a new data point for prediction\n",
    "new_data = np.array([13, -0.436905071360625, 0.918966212909322, 0.92459077438817,\n",
    "                     -0.727219053596792, 0.915678718106307, -0.127867352079254,\n",
    "                     0.707641607333935, 0.0879623554672504, -0.66527135413364,\n",
    "                     -0.737979823596458, 0.32409781346169, 0.277192107214981,\n",
    "                     0.252624256310781, -0.291896460370468, -0.184520169327133,\n",
    "                     1.14317370716197, -0.92870926272403, 0.680469592634687,\n",
    "                     0.0254364616880793, -0.0470212823165035, -0.194795823794671,\n",
    "                     -0.672637997017793, -0.156857514491897, -0.888386320943716,\n",
    "                     -0.342413218776576, -0.049026728633951, 0.0796923991551505,\n",
    "                     0.131023789452311, 0.89])  # Change \"0\" to 0 for numerical consistency\n",
    "\n",
    "# Reshaping the new data point\n",
    "new_data = new_data.reshape(1, -1)\n",
    "\n",
    "# Scaling the new data\n",
    "new_data_scaled = scaler.transform(new_data)\n",
    "\n",
    "# Making the prediction\n",
    "prediction = model.predict(new_data_scaled)\n",
    "print(\"Predicted Class:\", prediction)\n",
    "if(prediction[0]==0):\n",
    "    print('Legitimate')\n",
    "else:\n",
    "    print('fraudualnt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4f6d1c-c477-4c68-9334-91a73bc3ad8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
